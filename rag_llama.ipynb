{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -Uq sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xuan/miniconda3/envs/xuan_cuda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/xuan/miniconda3/envs/xuan_cuda/lib/python3.10/site-packages/torchvision/datapoints/__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "/home/xuan/miniconda3/envs/xuan_cuda/lib/python3.10/site-packages/torchvision/transforms/v2/__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [01:41<00:00, 50.67s/it]\n"
     ]
    }
   ],
   "source": [
    "chechpoint = \"/home/xuan/llama3.2-8b-train-py\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(chechpoint)\n",
    "llama_model = AutoModelForCausalLM.from_pretrained(chechpoint, torch_dtype=torch.float16)\n",
    "generator = pipeline(\"text-generation\", model=llama_model, tokenizer=tokenizer, device=\"cuda\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_snippets = [\n",
    "    \"Fiona thanked Ethan for his unwavering support and promised to cherish their friendship.\",\n",
    "    \"As they ventured deeper into the forest, they encountered a wide array of obstacles.\",\n",
    "    \"Ethan and Fiona crossed treacherous ravines using rickety bridges, relying on each other's strength.\",\n",
    "    \"Overwhelmed with joy, Fiona thanked Ethan and disappeared into the embrace of her family.\",\n",
    "    \"Ethan returned to his cottage, heart full of memories and a smile brighter than ever before.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "embeddings_text_snippets = model.encode(text_snippets)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_snippet(query):\n",
    "    query_embedded = model.encode([query])                                              # Encode the query to obtain its embedding\n",
    "    similarities = model.similarity(embeddings_text_snippets, query_embedded)           # Calculate cosine similarities between the query embedding and the snippet embeddings\n",
    "    retrieved_texts = text_snippets[similarities.argmax().item()]                       # Retrieve the text snippet with the highest similarity\n",
    "    return retrieved_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this step, we utilize the retrieved context snippets to generate a relevant answer using LLaMA, exemplifying the power of RAG in enhancing the quality of responses.\n",
    "\n",
    "def ask_query(query):\n",
    "    retrieved_texts = retrieve_snippet(query)\n",
    "\n",
    "    # Prepare the messages for the text generation pipeline\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"\n",
    "                \"Provide one Answer ONLY the following query based on the context provided below. \"\n",
    "                \"Do not generate or answer any other questions. \"\n",
    "                \"Do not make up or infer any information that is not directly stated in the context. \"\n",
    "                \"Provide a concise answer.\"\n",
    "                f\"{retrieved_texts}\"},\n",
    "        {\"role\": \"user\", \"content\": query}\n",
    "    ]\n",
    "\n",
    "    # Generate a response using the text generation pipeline\n",
    "    response = generator(messages, max_new_tokens=128)[-1][\"generated_text\"][-1][\"content\"]\n",
    "    print(f\"Query: \\n\\t{query}\")\n",
    "    print(f\"Context: \\n\\t{retrieved_texts}\")\n",
    "    print(f\"Answer: \\n\\t{response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: \n",
      "\tWhy did Fiona thank Ethan?\n",
      "Context: \n",
      "\tFiona thanked Ethan for his unwavering support and promised to cherish their friendship.\n",
      "Answer: \n",
      "\tFiona thanked Ethan for his unwavering support.\n"
     ]
    }
   ],
   "source": [
    "query = \"Why did Fiona thank Ethan?\"\n",
    "ask_query(query)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
